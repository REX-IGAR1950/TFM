{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO 1º CLUSTERIZACIÓN FRAGMENTADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "#### Este primer modelo tiene como objetivo clusterizar los puntos de reparto de mi base de datos para optimizar las rutas de transporte. Para ello me valdré de las siguientes librerías, algunas de las cuales no tendrán repercusión en el modelo. He querido mantener en cierta medida los pasos que he ido dando en el desarrollo del modelo para comunicar mis impresiones, así como las complicaciones que me he ido encontrando, ya que, ante todo, este proyecto tiene una finalidad didáctica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Librerias utilizadas para el desarrollo efectivo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "#libreria googlemaps para python:\n",
    "#! pip install -U googlemaps\n",
    "import googlemaps\n",
    "\n",
    "#Actualizar el pip instalador\n",
    "#! pip install --upgrade pip\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Librerías usadas pero no utilizadadas para el desarrollo efectivo del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Libreria geopy: geocodificar direcciones:\n",
    "\n",
    "! pip install geopy\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "**GEOPY**: Antes de empezar a trabajar con la plataforma de Google Maps, investigo acerca de esta librería de GEOPY, la cual geocodifica direcciones. El objetivo era aprender alguna manera alternativa a Google para geocodificar, ya que necesitaría sacar dos tipos de productos de Google (distance matrix y geocoding), lo cual tiene un coste. Tras varios intentos parece que funciona, aunque bastante regular dando ubicaciones sin sentido (ej:calle orense 32, Madrid)\n",
    "\n",
    "Nota: en el 2º Modelo veremos una tercera vía para geocodificar (GEOPANDAS) aunque también limita el número de peticiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ejemplo GEOPY\n",
    "- geolocator = Nominatim(user_agent=\"user_project\")\n",
    "- location = geolocator.geocode(\"Calle del Universo, 3, Valladolid\")\n",
    "- print((location.latitude, location.longitude))\n",
    "(41.6569033, -4.6990056)\n",
    "- print(location.raw)\n",
    "{'place_id': '93034279', 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright', 'osm_type': 'way', 'osm_id': '75689481', 'boundingbox': ['41.6550687', '41.6587924', '-4.6990298', '-4.698207'], 'lat': '41.6569033', 'lon': '-4.6990056', 'display_name': 'Calle del Universo, Pilarica, Valladolid, Castilla y León, 47011, España', 'class': 'highway', 'type': 'residential', 'importance': 0.8099999999999999}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INICIO: LECTURA Y SELECCIÓN DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos vienen en formato excel que es el que suelen usar en la empresa de transporte. La información que nos interesa es la relativa a la dirección (dirección,CP,Población) y fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = pd.read_excel('data_prueba.xls', usecols=[5, 6,7,15], convert_float=True, skip_footer=2, parse_dates=['FECHA REPARTO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creo una columna unificando las 3 columnas relativas a dirección para mejor entendimiento de la API de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max['DESTINO'] = df_max[['DIRECCION','CP DESTINO','POBLACION DESTINO']].apply(lambda x : '{}, {}, {}'.format(x[0],x[1],x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_max.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida es un dataframe sobre el que trabajaremos para cumplir el objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FILTRADO POR FECHA DE REPARTO. En este caso el 04/02/2019 es la que presenta el grueso de repartos\n",
    "date_of_interest = pd.datetime(2019,2,4)\n",
    "df_max = df_max[ df_max['FECHA REPARTO']==date_of_interest]\n",
    "df_max.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVIDO DE DUPLICADOS**: En la base de datos es muy común que haya más de una entrega en la misma dirección, bien por consistir en diferente tipo de entrega (paletizado/mensajería) o simplementa que coincida el destino. Como el objetivo es optimizar la ruta, lo que haremos será quedarnos con los destinos borrando los duplicados. Así mismo se pasará al jefe de trafico los destinos duplicados para que lo tenga en cuenta en la carga de mercancía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUPLICADOS EN LAS ENTREGAS:\n",
    "\n",
    "same_directions = np.array([],dtype=str)#datos a sacar por pantalla al jefe de tráfico en Tableau\n",
    "differents_directions = np.array([],dtype=str)\n",
    "\n",
    "for i in df_max['DESTINO']:            \n",
    "        if i not in differents_directions:\n",
    "                differents_directions = np.append(differents_directions,i)\n",
    "        else:\n",
    "            same_directions = np.append(same_directions,i)  \n",
    "            #print('ten en cuenta que la dirección %s tiene más de una entrega' % same_direction)\n",
    "            \n",
    "# REMOVIDO DE ENTREGAS DUPLICADAS EN EL MISMO DESTINO PARA LIMPIAR EL DATAFRAME\n",
    "df_max.drop_duplicates(inplace=True)\n",
    "df_max.reset_index(drop=True,inplace=True)\n",
    "len(df_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE MAPS PLATFORM\n",
    "\n",
    "**LOGGIN**: obtengo una cuenta y un proyecto con Google Maps, lo cual me permitirá aprovechar sus utilidades. Hay que epsecificar los servicios que se requeriran ya que son llamadas de API diferenciadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucción al examinador: la API Key la guardo en un documento .py la cual importo, este documento se envía por correo electrónico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from info_tfm import api_key\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_google = googlemaps.Client(key=api_key)\n",
    "user_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accedo a un REPOSITORIO abierto de desarrolladores de google maps para PYTHON, la cual me brinda la posibilidad de poder trabajar desde Python y acceder a los direferentes servicios Google Maps Platform tiene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repositorio con la documentación: https://googlemaps.github.io/google-maps-services-python/docs/#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTANCE MATRIX \n",
    "\n",
    "#### El primer objetivo es crear la matriz de distancias entre los diferentes destinos de entrega. Google aporta una serie de servicios enre los que se incluye la llamada \"distance matrix\" (con una llamada a la API especifica para ello). Esta es la que usaré a continuación.  En este servicio Google geocodifica por defecto para poder sacar los siguientes parámetros:\n",
    "\n",
    "- Distancia\n",
    "- Duración\n",
    "- Duración en tráfico (si el transporte es vehiculo a motor)\n",
    "- Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlemaps import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tengo que hacer chunks de reiteración a la API de Google\n",
    "\n",
    "##### Dado que la API de Google no tolera más de 10 peticiones de combinaciones de origen y destino, y no tiene limite de peticiones diarias, voy a hacer iteraciones de 10 en 10 peticiones\n",
    "\n",
    "Condiciones Google:\n",
    "\n",
    "- Limited to 100 elements per client-side request.\n",
    "- Maximum of 25 origins and 25 destinations per server-side request.\n",
    "- 1,000 server-side elements per second. *Note that the client-side service offers Unlimited elements per second, per project.\n",
    "\n",
    "Para más información: https://developers.google.com/maps/documentation/javascript/distancematrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARÁMETROS DE LA DISTANCE MATRIX DE GOOGLE\n",
    "- Mode=driving ---> porque nuestros repartos son en vehiculo en su mayor parte\n",
    "- Departure_time = 'now'---> Debido a que la orden de repartos es en la mañana\n",
    "- traffic_model = 'pessimistic'---> para no penalizar al conductor si la ruta sufre más atascos de lo normal\n",
    "- region = '.es'---> usamos la ccTLD (códigos de pais) de España para prevenir duplicidades con direcciones de paises hispanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de la clusterización\n",
    "\n",
    "Se presenta un problema y es que no se puede sacar una matriz de distancias general para todos lo puntos de reparto si superan los 10 destinos, ya que, debido a esta limitación de Google, no se podrán conectar todos los puntos de reparto entre ellos, quedando una matriz de distancias incompleta.\n",
    "\n",
    "***NOTA***: en el segundo modelo solventaré esta situación y conseguiré una matriz de distancias completa.\n",
    "\n",
    "#### Es por ello que, para tratar de darle continuidad a esta limitación inicial, trataré cada grupo de 10 combinaciones de entregas como un cluster cerrado. Al final quedarán n clusteres de máximo 10 puntos de reparto (destinos) cada uno. A este paso lo llamaré clusterización de 1º grado\n",
    "\n",
    "**Solución**: Para crear un objeto con todos los clusteres que devuelve google (de máximo 10 destinos cada) iteraremos la función de matriz de distancias por bloques de 10 destinos correlativamente sobre la columna de DESTINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLE Nº DESTINOS\n",
    "\n",
    "La variable \"n\" tiene en cuenta el número de destinos a tomar en cuenta en el modelo. En mi caso cogeré 60 para ambos modelos por cuestiones económicas y de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_max.head(n=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comunicar que nuestro modelo presenta un coste mayor al usual de petición a Google ya que el parámetro **traffic_model** eleva el precio de petición a Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERIZACIÓN 1º GRADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#origenes y destinos con corte maximo de 10\n",
    "\n",
    "interval = 10\n",
    "c_size = 1\n",
    "dict_clusters={}\n",
    "\n",
    "for i in range(math.ceil(len(df)/interval)):# importo math para tener la función ceil que redondea al alza\n",
    "        \n",
    "        df_chunk = df.loc[c_size:interval]\n",
    "        dm = distance_matrix.distance_matrix(client=user_google, origins = df_chunk['DESTINO'],\n",
    "                                     destinations = df_chunk['DESTINO'], mode = 'driving',\n",
    "                                     departure_time = 'now', traffic_model = 'pessimistic', \n",
    "                                     region = '.es')\n",
    "        dict_clusters[i] = dm\n",
    "        c_size+= 10\n",
    "        interval+=10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros de salida\n",
    "\n",
    "- Valor distance: magnitud en metros\n",
    "- Valor duration: magnitud en segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el bucle anterior no me permite pasarlo a lista directamente, de ahí que se separe\n",
    "list_clusters = []\n",
    "\n",
    "for i in dict_clusters.keys():\n",
    "    \n",
    "    list_clusters.append(dict_clusters[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESANIDAMIENTO DE LA INFORMACIÓN DE GOOGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto tenemos una lista de clusters con el formato sacado directamente de Google. Ese formato presenta una estructura de listas y diccionarios superspuestos de donde tendré que sacar la información para crear mi Dataframe con un formato más homogéneo.\n",
    "\n",
    "Google devuelve las siguientes Keys de donde parten los datos:\n",
    "\n",
    "- origin_addresses\n",
    "- destination_addresses\n",
    "- rows: dentro viene la información relativa a tiempo y duración, así como duración de tráfico y status\n",
    "- status\n",
    "\n",
    "**NOTA**: CUANDO EL STATUS ES *NOT FOUND* NO HAY SALIDA DE LOS PARAMETROS, POR LO QUE NO COINCIDEN LAS LONGITUDES DE LAS SERIES QUE CONFORMAN EL DATAFRAME. POR ELLO CREO UN DATA FRAME INICIAL AL QUE QUITAMOS LAS LINEAS \"*NOT FOUND*\" PARA QUE LUEGO PUEDA PASAR LOS OTROS PARÁMETROS (DISTANCE, DURATION Y DURATION IN TRAFFIC) Y SE COLOQUEN EN EL ORDEN DESEADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESANIDAMIENTO\n",
    "\n",
    "clusters_df = []\n",
    "\n",
    "for cluster in range(len(list_clusters)):\n",
    "\n",
    "    origen=[]\n",
    "\n",
    "    for key,values in list(list_clusters[cluster].items()):    \n",
    "        if key=='origin_addresses': \n",
    "            for i in values:\n",
    "                for n in range(len(values)):            \n",
    "                    s1=origen.append(i)\n",
    "\n",
    "    for i in list_clusters[cluster].keys():    \n",
    "        if i=='destination_addresses':\n",
    "            destino = list_clusters[cluster][i]\n",
    "            destino = destino * len(list_clusters[cluster][i])\n",
    "            \n",
    "    distance=[]\n",
    "    duration=[]\n",
    "    duration_in_traffic=[]\n",
    "    status=[]\n",
    "\n",
    "    for d in list_clusters[cluster].keys():    \n",
    "        if d=='rows':        \n",
    "            x2= list_clusters[cluster][d]\n",
    "\n",
    "    # 1) en esta fase se hace diccionario de la lista previa con los 3 elementos\n",
    "    for l in x2:\n",
    "\n",
    "        # 2) en esta fase se hace lista de los valores del diccionario anterior \n",
    "            for l1 in l.values():\n",
    "\n",
    "                # 3) en esta fase se hace diccionario de las partes de la lista anterior   \n",
    "                for l2 in l1:\n",
    "                  \n",
    "                    for key,values in list(l2.items()):   \n",
    "\n",
    "                        if key=='distance':\n",
    "                            for key,values in list(values.items()):\n",
    "                                if key=='value':\n",
    "                                    distance.append(values)\n",
    "                        if key=='duration':\n",
    "                            for key,values in list(values.items()):\n",
    "                                if key=='value':\n",
    "                                    duration.append(values) \n",
    "                        if key=='duration_in_traffic':\n",
    "                            for key,values in list(values.items()):\n",
    "                                if key=='value':\n",
    "                                    duration_in_traffic.append(values)\n",
    "                        if key=='status':\n",
    "                            status.append(values)\n",
    "                            \n",
    "    #Creación de cluster en Data Frame\n",
    "    dfdata = { 'origen' : origen,\n",
    "               'destino' : destino,\n",
    "               'status': status\n",
    "               }\n",
    "        \n",
    "    ddm = pd.DataFrame(dfdata,columns=['origen','destino','status'])\n",
    "    ddm = ddm[ ddm['status']!='NOT_FOUND']# Para evitar nulos y posibilitar que la extensión de las arrays coincidan \n",
    "    ddm['distance'] = distance\n",
    "    ddm['duration'] = duration\n",
    "    ddm['duration_in_traffic'] = duration_in_traffic\n",
    "    clusters_df.append(ddm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**: se comprueba que en algunas ocasiones (muy pocas realmente) Google a veces no devuelve el parámetro **duration in traffic**, el cual es el que queremos tener en cuenta en el modelo. Por ello y para que no de error el modelo nos fijamos en el parámetro **duration** (por si acaso Google falla). Es por ello que en el modelo 2 capo la posibildiad de crear el dataframe con \"duration in traffic\" ya que a veces daba error en la longitud de las arrays de salida. \n",
    "\n",
    "**En este modelo 1º no mantengo capada la salida de duration in traffic porque relamente no será nuestro modelo, así que trabajo sobre duration in traffic para que sea vea otra manera alternativa de recopilación de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZACIÓN INSIDE CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### El siguiente paso es optimizar las rutas dentro de cada cluster.\n",
    "\n",
    "Tras probar AgglomerativeClustering y scipy.cluster.hierarchy tomo una de las decisiones más importantes de mi TFM y es la de intentar crear mi propio algoritmo de optimización y clusterización de las rutas en base a las variables que estimemos (duration, duration in traffic y distance).\n",
    "\n",
    "**CLUSTERIZACIÓN**: Como observé en el transcurso del proyecto, dado el tipo de limitación de Google y la tipología de la finalidad buscada (optimizar la ruta), los algoritmos usuales de clusterización no se ajustaban a lo que pretendía. Lo que necesito es un linkage tipo ***single*** que tome en consideración que el punto de partida desde donde medir la distancia al punto más cercano es el último destino. Esto es debido a que en la realidad un repartidor llega a un destino y sólo puede partir de ahí para el siguiente punto más cercano, por lo que ni los modelos ward, complete, los tipo de promedio o incluso el single básico no se ajustarán a la realidad de las rutas.\n",
    "\n",
    "Entre este motivo y que tras pasar la distance matrix ordenada (y no simétrica), no conseguía estructurar la salida de los clusters usando los algoritmos hierarchy (relativo a origenes y destinos) tomo la decisión de crear mi código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÓDIGO TABLA OPTIMIZACIÓN INTERNA DE LA LISTA DE CLUSTERES\n",
    "\n",
    "def optimizer_cluster(clusters_df,var_optimizer='duration'):\n",
    "    \n",
    "    # variables iniciales\n",
    "    index_origin = clusters_df[var_optimizer].idxmin()\n",
    "    destination = clusters_df.loc[index_origin][0]\n",
    "    df_exit = pd.DataFrame(columns=('origen','destino','distance','duration','duration_in_traffic'))\n",
    "\n",
    "    while len(clusters_df)>0:\n",
    "   \n",
    "            index = clusters_df[ clusters_df['origen']==destination][var_optimizer].idxmin()\n",
    "            df_exit = df_exit.append((clusters_df.loc[index]).T)\n",
    "            origin = clusters_df.loc[index][0]\n",
    "            destination = clusters_df.loc[index][1]\n",
    "            clusters_df.drop(index,inplace=True) \n",
    "            clusters_df.drop(clusters_df  [ (clusters_df['origen']==destination) & (clusters_df['destino']==origin)].index,inplace=True)\n",
    "            clusters_df.drop(clusters_df  [  clusters_df['destino']==destination].index,inplace=True)\n",
    "            clusters_df.drop(clusters_df  [  clusters_df['destino']==origin].index,inplace=True)\n",
    "        \n",
    "    return df_exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paso intermedio de removido de distancias nulas y de la columna status (inservible ya).\n",
    "for i in range(len(clusters_df)):\n",
    "        clusters_df[i] = clusters_df[i] [(clusters_df[i]['distance']!=0)]\n",
    "        clusters_df[i].reset_index(drop=True,inplace=True)\n",
    "        clusters_df[i].drop('status',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una copia para darle continuidad a partir de aquí (para gestionar posibles fallos que se daban y no repetir el código)\n",
    "clusters_df_optimazed = clusters_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LE PASAMOS LA FUNCIÓN OPTIMIZER A CADA CLUSTER PARA CONOCER LA RUTA MÁS OPTIMA\n",
    "clusters_df_optimazed = list(map(optimizer_cluster,(clusters_df_optimazed)))\n",
    "clusters_df_optimazed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df_optimazed[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERIZACIÓN 2º GRADO\n",
    "\n",
    "##### Recapitulación: hasta ahora lo que hemos hecho es crear clusters de máximo 10 entregas cada uno (n<=10), obteniendo una lista de clusters a la que pasamos la función optimizer. Al pasarle esta función lo que hacemos es imitar el algoritmo de clusterización hierarchycal clustering, usando el equivalente al linkage \"single\" pero adecuado a realidad de un repartidor, en la que el punto de partida es siempre el de la última entrega. \n",
    "\n",
    "### Lo que vamos a hacer ahora es intentar conectar los origenes y destinos de estos clusteres ya creados para seguir agrupando de la manera más eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada cluster ya calculado (clusterización de 1º grado) se conectará con el resto de clusters. Para ello, el destino de un cluster de 1º grado (inside_cluster) sería el punto de partida (origen) desde el que conectar con otro cluster de 1º grado. Asímismo, los origenes de los clusteres de 1º grado serían los posibles destinos a los que conectar. Es por ello que, de forma resumida, se invierten los puntos origenes y destinos de la clusterización de 1º grado, pasando estos a ser destinos y origenes en su conexión al resto de clusteres (clusterización de 2º grado/outside_cluster). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resalto que esta parte puede resultar algo complicada de entender porque lo que antes era último destino inside_cluster ahora se convierte en origen outside_cluster y viceversa. Usaremos ambas definiciones según estemos inside_cluster u outside_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EN ESTE PASO INVIERTO LOS ORIGENES,DESTINOS DENTRO DE CADA CLUSTER CREANDO UN DF PARA CONOCER LAS CONEXIONES 2º GRADO\n",
    "\n",
    "origenes = {} #los usaremos  para dropear luego\n",
    "destinos = {}\n",
    "\n",
    "for cluster in range(len(clusters_df_optimazed)):\n",
    "    \n",
    "    for i in list(clusters_df_optimazed[cluster]['destino'].tail(1)):\n",
    "        origenes[cluster] = i\n",
    "\n",
    "    for i in list(clusters_df_optimazed[cluster]['origen'].head(1)):\n",
    "        destinos[cluster] = i\n",
    "\n",
    "    \n",
    "dfdata = { 'origen' : origenes,\n",
    "               'destino' : destinos,\n",
    "               }\n",
    "\n",
    "df_ori_des = pd.DataFrame(dfdata,columns=['origen','destino'],dtype=str)\n",
    "df_ori_des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DICCIONARIOS ÚTILES PARA ULTERIORES FUNCIONES\n",
    "\n",
    "Creamos diccionarios de origen y destino **\"inside cluster\"** y **\"outsidecluster\"** para ver las consexiones.\n",
    "\n",
    "Peligro: No pueden coincidir rows de origenes y destinos porque sino el diccionario descarta el duplicado (ya que actúa como un set). No debería haber duplicados porque los hemos quitado, pero Google puede crear duplicados al geocodificarlos (resumiendo destinos a Codigo Postal y población)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_des_outsidecluster = dict(zip(origenes.values(),destinos.values()))\n",
    "ori_des_insidecluster = dict(zip(destinos.values(),origenes.values()))\n",
    "ori_des_outsidecluster,ori_des_insidecluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EL SIGUIENTE PASO ES OBTENER LA DISTANCE MATRIX ENTRE LOS DIFERENTES CLUSTERES PARA OBTENER LA MEJOR AGRUPACIÓN DE ELLOS\n",
    "\n",
    "dm_clusters = distance_matrix.distance_matrix(client=user_google, origins = df_ori_des['origen'],\n",
    "                                     destinations = df_ori_des['destino'], mode = 'driving',\n",
    "                                     departure_time = 'now', traffic_model = 'pessimistic', \n",
    "                                     region = '.es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESANIDAMIENTO Y OPTIMIZACIÓN CLUSTERIZACIÓN 2º GRADO\n",
    "\n",
    "Similar al paso de la 1º Clusterización pero pero el objetivo es diferente. En este caso queremos crear un dataframe que me genere las pautas de conexión entre los clusteres iniciales, para, poseteriomente (como veremos), agruparlos optimizando según las variables que estimemos (distance, duration, duration in traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de desanidamiento varía con el anterior ya que en este no iteramos sobre más clusteres. No he querido ampliar el modelo a más de 100 destinos (en dado caso, debido a la limitación de Google debería crear lista de clusters como en la clusterización de 1º Grado) para cerrar antes este modelo. No me aportaba más creatividad y sí mucha codificación repetida, a la par que no es el modelo escogido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformación del código en data frames\n",
    "\n",
    "origen=[]\n",
    "\n",
    "for key,values in list(dm_clusters.items()):    \n",
    "    if key=='origin_addresses': \n",
    "        for i in values:\n",
    "            for n in range(len(values)):            \n",
    "                s1=origen.append(i)\n",
    "\n",
    "for i in dm_clusters.keys():    \n",
    "    if i=='destination_addresses':\n",
    "        destino = dm_clusters[i]\n",
    "        destino = destino * len(dm_clusters[i])\n",
    "\n",
    "distance=[]\n",
    "duration=[]\n",
    "duration_in_traffic=[]\n",
    "status=[]\n",
    "\n",
    "for d in dm_clusters.keys():    \n",
    "    if d=='rows':        \n",
    "        x2= dm_clusters[d]\n",
    "\n",
    "# 1) en esta fase se hace diccionario de la lista previa con los 3 elementos\n",
    "for l in x2:\n",
    "\n",
    "    # 2) en esta fase se hace lista de los valores del diccionario anterior \n",
    "        for l1 in l.values():\n",
    "\n",
    "            # en esta fase se hace diccionario de las partes de la lista anterior   \n",
    "            for l2 in l1:\n",
    "\n",
    "\n",
    "                for key,values in list(l2.items()):   \n",
    "\n",
    "                    if key=='distance':\n",
    "                        for key,values in list(values.items()):\n",
    "                            if key=='value':\n",
    "                                distance.append(values)\n",
    "                    if key=='duration':\n",
    "                        for key,values in list(values.items()):\n",
    "                            if key=='value':\n",
    "                                duration.append(values) \n",
    "                    if key=='duration_in_traffic':\n",
    "                        for key,values in list(values.items()):\n",
    "                            if key=='value':\n",
    "                                duration_in_traffic.append(values)\n",
    "\n",
    "                    if key=='status':\n",
    "                        status.append(values)\n",
    "\n",
    "#Creación de cluster en Data Frame\n",
    "dfdata_clusters = { 'origen' : origen,\n",
    "           'destino' : destino,\n",
    "           'status': status\n",
    "           }\n",
    "\n",
    "ddm_clusters = pd.DataFrame(dfdata_clusters,columns=['origen','destino','status'])\n",
    "ddm_clusters = ddm_clusters[ ddm_clusters['status']!='NOT_FOUND']# Para evitar nulos y posibilitar \n",
    "#que la extensión de las arrays coincidan. En este caso es insospechado que pueda haber NOT FOUNDS\n",
    "ddm_clusters['distance'] = distance\n",
    "ddm_clusters['duration'] = duration\n",
    "ddm_clusters['duration_in_traffic'] = duration_in_traffic\n",
    "#exec(df_cluster{i} = ddm).format(i)\n",
    "ddm_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddm_clusters.drop('status',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos, este dataframe de distance matrix también cruza los origenes y destinos propios de cada cluster de 1º grado (inside_cluster). Estos los tenemos que quitar para no tenerlos en cuenta en la optimización posterior ya que un cluster de 1º grado no se va a conectar a él mismo. Es lo que hacemos en el código posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Los origenes y destinos internos de cada cluster no tienen que tenerse en cuenta para el linkage\n",
    "\n",
    "for cluster in range(len(df_ori_des)):\n",
    "        print(origenes[cluster])\n",
    "        ddm_clusters.drop(ddm_clusters  [ (ddm_clusters['origen']==origenes[cluster])\n",
    "                                     & (ddm_clusters['destino']==destinos[cluster])].index,inplace=True)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddm_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una copia para darle continuidad a partir de aquí (para gestionar posibles fallos que se daban y no repetir el código)\n",
    "ddm_clusters_optimized = ddm_clusters.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIMIZACIÓN\n",
    "\n",
    "Optimización de esta **clusterización de 2º Grado** para conectar los clusteres de 1º grado. Lo que se pretende es conectar los cluteres así que lo que hacemos es hacerlo por el linkage single de menor distancia entre los origenes y destinos\n",
    "\n",
    "Se diferencia del primero en la estructura de origenes y destinos por lo que **no** se hace con la **función optimizer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Tanto en este paso (como en el similar del 1º grado) se vacía el dataframe que sirve de base, lo que tendrá consecuencias de no retroceso en el proceso. Esto es porque lo que hacemos es dropear las lineas que ya se han incluido en el nuevo dataframe optimizado (a la par que dropeamos las conexiones que ya no se darán)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "index_origin = ddm_clusters_optimized['duration_in_traffic'].idxmin()\n",
    "destination = ddm_clusters_optimized.loc[index_origin][0]\n",
    "df_exit = pd.DataFrame(columns=('origen','destino','distance','duration','duration_in_traffic'))\n",
    "\n",
    "while len(ddm_clusters_optimized)>0:\n",
    "    \n",
    "    index = ddm_clusters_optimized[ ddm_clusters_optimized['origen']==destination]['duration_in_traffic'].idxmin()\n",
    "    df_exit = df_exit.append((ddm_clusters_optimized.loc[index]).T)\n",
    "    destination_initial = ddm_clusters_optimized.loc[index][1]\n",
    "    origin_initial = ddm_clusters_optimized.loc[index][0]\n",
    "    ddm_clusters_optimized.drop(ddm_clusters_optimized  [  ddm_clusters_optimized['destino']==destination_initial].index,inplace=True)\n",
    "    ddm_clusters_optimized.drop(ddm_clusters_optimized  [  ddm_clusters_optimized['origen']==origin_initial].index,inplace=True)\n",
    "    ddm_clusters_optimized.drop(ddm_clusters_optimized  [  ddm_clusters_optimized['destino']==ori_des_outsidecluster[origin_initial]].index,inplace=True)\n",
    "    destination = ori_des_insidecluster[destination_initial]\n",
    "    \n",
    "df_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente lo que nos interesa es el orden de conexión de los clusteres de 1º Grado, por lo que en el siguiente código sacamos una lista con el orden de recorrido de los clusteres (tras optimizar sus conexiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una lista con el orden de origen-destinos necesario para ordenar posteriormente los clusters\n",
    "df_intercluster_exit = []\n",
    "\n",
    "for i in range(len(df_exit)):\n",
    "    if i ==0:\n",
    "        df_intercluster_exit.append(df_exit.iloc[0]['origen'])\n",
    "    df_intercluster_exit.append(df_exit.iloc[i]['destino'])\n",
    "\n",
    "df_intercluster_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo con lo indicado, saco el orden de conexión de los clusteres de 1º grado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZACIÓN DE LOS CLUSTERES\n",
    "\n",
    "orden_clusteres_list = []\n",
    "\n",
    "for i in range(len(df_intercluster_exit)):\n",
    "    if i==0:\n",
    "        orden_clusteres_list.append(list(df_ori_des [df_ori_des['origen'] == df_intercluster_exit[i]].index))\n",
    "    else:\n",
    "        orden_clusteres_list.append(list(df_ori_des [df_ori_des['destino'] == df_intercluster_exit[i]].index))\n",
    "        \n",
    "orden_clusteres_int = []\n",
    "\n",
    "for order in orden_clusteres_list:\n",
    "    for i in order:\n",
    "        orden_clusteres_int.append(i)\n",
    "        \n",
    "orden_clusteres_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARACIÓN DATAFRAME FINAL\n",
    "\n",
    "##### Reorganización de los clusters de 1º grado en función del resultado de la clusterización de 2º grado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_clusters_optimazed = pd.DataFrame()\n",
    "\n",
    "for order in orden_clusteres_int:\n",
    "    total_clusters_optimazed = total_clusters_optimazed.append(clusters_df_optimazed[order])\n",
    "\n",
    "#Reseteo para quitarme los index antiguos de la llamada a la API.\n",
    "total_clusters_optimazed.reset_index(drop=True)\n",
    "\n",
    "total_clusters_optimazed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASE CLUSTERIZACIÓN 3º GRADO\n",
    "\n",
    "En este momento disponemos de un conjunto ordenado por la variable elegida (duration en nuestro caso) optimizadas anteriormente. La clusterización  de 3º grado consiste en reorganizar los clusters atendiendo al tiempo de reparto de los conductores. Es por ello que segmentaremos el dataframe en clusteres atendiendo a la variable **tiempo en ruta**\n",
    "\n",
    "#### Técnicamente, lo que hemos hecho en los procesos anteriores es clusterizar sin seguir un criterio. Sólo nos limitábamos a a seguir las limitaciones de Google para luego solventar esta limitación en el proceso de 2º Clusterización\n",
    "\n",
    "Lo que nos proponemos ahora es fijar nuestro criterio para clusterizar. Este criterio o variable limitará el número de destinos o rutas por cluster en función de que se cumpla el argumento que queramos. Esta variable es el **tiempo en ruta** que queremos asignar a cada reaprtidor para que se ajuste a su horario\n",
    "\n",
    "Esta fase es la clave para comprender la finalidad de nuestro modelo,  que tiene tanto una vertiente económica relacionada con la optimización del trabajo de cada repartidor, como decalidad del servicio otorgado\n",
    "\n",
    "#### Es por ello que la métrica comparativa entre modelos será el tiempo estimado de las rutas totales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CÓDIGO DE CLUSTERIZACIÓN ATENDIENDO AL CRITERIO ESPECIFICADO\n",
    "\n",
    "Creo la función **my_cumsum_func** que genera clusters nuevos en el momento en el que la duración de la ruta supera el tiempo fijado de reparto de nuestros conductores\n",
    "\n",
    "En esta parte ya empiezo a generar arrays de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cumsum_func(column_duration,time_route=21600):\n",
    "        grp = np.zeros(len(column_duration))\n",
    "        grp[0] = 0\n",
    "        #dfdata = { 'cumsum_duration' : column_duration,'clusters': grp }\n",
    "\n",
    "        for i in range(1,len(column_duration)):\n",
    "\n",
    "            if (column_duration[i-1] + column_duration[i]) <= time_route:\n",
    "                grp[i] = grp[i-1]\n",
    "                column_duration[i] = column_duration[i-1] + column_duration[i]\n",
    "            else:\n",
    "                grp[i] = grp[i-1] + 1\n",
    "                \n",
    "        result = np.array([column_duration,grp])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para reiniciar la prueba del orden de clusteres\n",
    "column_duration = np.array(total_clusters_optimazed['duration_in_traffic'])\n",
    "len(column_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APLICO LA FUNCIÓN CUMSUM SOBRE LA DURACIÓN EN TRÁFICO\n",
    "array_cumsum = my_cumsum_func(column_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUMENTO DEL DATAFRAME TOTAL CON LOS CLUSTERES Y EL CUMSUM\n",
    "total_clusters_optimazed['cumsum_duration']=array_cumsum[0]\n",
    "total_clusters_optimazed['clusters']=array_cumsum[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(total_clusters_optimazed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clusters_optimazed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clusters_optimazed['duration'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALISIS DE MÉTRICAS\n",
    "\n",
    "### El apartado del análisis de las metricas entre los dos modelos radicará en comparar la variable *duration*. Aquel modelo que mejor se comporte será el que implementemos la puesta en marcha para la visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipotesis: la prueba comparativa tendrá en cuenta para ambos modelos:\n",
    "\n",
    "- Variable escogida: duration (ya que duration en traffic hemos comprobado que a veces Google no devuelve el valor)\n",
    "- Nº de repartos/destino = 60 (para ambos modelos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1º Modelo: total_clusters_optimazed['duration'].sum() = 26440\n",
    "\n",
    "2º Modelo: df_exit['duration'].sum() = 17963\n",
    "\n",
    "Como era de esperar, se reduce más el tiempo en el segundo modelo, ya que pone en relación directa a todos los destinos, pudiendo conformar una DISTANCE MATRIX unica para todos.\n",
    "En detrimento de esta optimización del tiempo se puede ver un peor comprotamiento del segundo modelo relativo al tiempo de resuesta de la API de Google así como a diversas complicaciones en el algoritmo de linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTA: este anáisis es estático de la prueba en cuestión. No coincide siempre el análisis ya que google varía las duraciones e incluso las direcciones sin causa aparente en algunos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN DEL MODELO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mantengo código inservible pero de prueba inicial de clusterización con los algoritmos de las librerias existentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESO DE CLUSTERIZACIÓN (no del modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from pylab import rcParams\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "#para que no nos salga la notación cientifca y nos salga 4 decimales\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para ver posibles outlier que no cumplan una relación distancia-tiempo adecuada\n",
    "plt.scatter(ddm['distancia'], ddm['duration'], s=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ddm_cluster=ddm.pivot_table(index='origen', columns='destino',values='duration')\n",
    "ddm_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PASAR UNA ARRAY O UN DATAFRAME?\n",
    "\n",
    "Podría convertir el tipo dataframe anterior a tipo array:\n",
    "ddm_cluster = ddm_cluster.values()\n",
    "El resultado de la clusterización Z matrix) es el mismo por lo que sigo pasandole un dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ddm_cluster_array = ddm_cluster.values\n",
    "ddm_cluster_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ddm_cluster_array[2, 0]\n",
    "# con la array puedo seleccionar el valor. Posible uso para la clusterización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USAMOS OTRA LIBRERIA PARA PROBAR: AgglomerativeClustering\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import \n",
    "\n",
    "#### define the model\n",
    "cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward',connectivity= ddm_cluster) #### connectivity \n",
    "\n",
    "#### fit data and predict \n",
    "clusters = cluster.fit_predict(ddm_cluster)\n",
    "plt.plot(ddm_cluster, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z = linkage(ddm_cluster,'single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "c = dendrogram(Z, truncate_mode = 'lastp', p=12, leaf_rotation=-25.,leaf_font_size=15., show_contracted=True)\n",
    "\n",
    "plt.title('Truncated Hierarchical Clustering Dendogram')\n",
    "plt.xlabel('delivery points')\n",
    "plt.ylabel('time-distance')\n",
    "\n",
    "\n",
    "plt.axhline(y=900)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plt.savefig('plot_dendrogram.png') . Para guardad el dendograma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pruebo una DEFINICIÓN que saqué de https://stackoverflow.com/questions/11917779/\n",
    "#how-to-plot-and-annotate-hierarchical-clustering-dendrograms-in-scipy-matplotlib\n",
    "\n",
    "def augmented_dendrogram(*args, **kwargs):\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            plt.plot(x, y, 'ro')\n",
    "            plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -8),\n",
    "                         textcoords='offset points',\n",
    "                         va='top', ha='center')\n",
    "\n",
    "    return ddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pruebo otra DEFINICIÓN: https://stackoverrun.com/es/q/4593125\n",
    "\n",
    "def plot_tree(P, pos=None): \n",
    "    icoord = np.array(P['icoord']) \n",
    "    dcoord = np.array(P['dcoord']) \n",
    "    color_list = np.array(P['color_list']) \n",
    "    xmin, xmax = icoord.min(), icoord.max() \n",
    "    ymin, ymax = dcoord.min(), dcoord.max() \n",
    "    if pos: \n",
    "        icoord = icoord[pos] \n",
    "        dcoord = dcoord[pos] \n",
    "        color_list = color_list[pos] \n",
    "    for xs, ys, color in zip(icoord, dcoord, color_list): \n",
    "        plt.plot(xs, ys, color) \n",
    "    plt.xlim(xmin-10, xmax + 0.1*abs(xmax)) \n",
    "    plt.ylim(ymin, ymax + 0.1*abs(ymax)) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#define the model\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n",
    "\n",
    "#fit data and predict \n",
    "clusters = cluster.fit_predict(ddm_cluster)\n",
    "clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
