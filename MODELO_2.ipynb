{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO 2º  CLUSTERIZACIÓN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "#### Siguiendo el mismo objetivo que el primer modelo trataré de clusterizar los repartos de la base de datos inicial. \n",
    "En principio las librerías serán las mismas, aunque en este segundo modelo implementaré un apartado de **Visualización** con Geopandas y Matplotlib (posteriormente con TABLEAU) basado en las geocodificaciones de la API de Google. En el primer modelo no lleve a cabo esta visualización porque requería más recursos de Google al hacer llamadas a la API \" geocoding\" lo cual supone un coste.\n",
    "\n",
    "Mencionar que la llamada a la **Distance Matrix** de Google saca por defecto las *geocodificaciones* aunque no las saca como output, motivo por el cual es imprescindible llamar posteriormente al servicio de Google \"Geocoding\" para poder visualizar basandonos en las latitudes y longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "#libreria googlemaps para python:\n",
    "#! pip install -U googlemaps\n",
    "import googlemaps\n",
    "\n",
    "#Actualizo el pip instalador\n",
    "#! pip install --upgrade pip\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INICIO: LECTURA Y SELECCIÓN DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos vienen en formato excel que es el que suelen usar en la empresa de transporte. La información que nos interesa es la relativa a la dirección (dirección,CP,Población) y fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_max = pd.read_excel('data_prueba.xls', usecols=[5, 6,7,15], convert_float=True, skip_footer=2, parse_dates=['FECHA REPARTO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creo una columna unificando las 3 columnas relativas a dirección para mejor entendimiento de la API de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max['DESTINO'] = df_max[['DIRECCION','CP DESTINO','POBLACION DESTINO']].apply(lambda x : '{}, {}, {}'.format(x[0],x[1],x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTRADO POR FECHA DE REPARTO. En este caso el 04/02/2019\n",
    "date_of_interest = pd.datetime(2019,2,4)# a solicitar al jefe de tráfico\n",
    "df_max = df_max[ df_max['FECHA REPARTO']==date_of_interest]\n",
    "df_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# REMOVIDO DE ENTREGAS DUPLICADAS EN EL MISMO DESTINO, YA QUE LAS ENTREGAS SE SUELEN DUPLICAR POR EL Nº DE ENVÍOS, LO QUE \n",
    "# PARA NUESTRO CASO NO VAMOS A TENER EN CUENTA A LA HORA DE LA CLUSTERIZACIÓN\n",
    "#AVISO DE DUPLICADOS EN LAS ENTREGAS:\n",
    "\n",
    "same_directions = np.array([],dtype=str)#datos a sacar por pantalla al jefe de tráfico\n",
    "differents_directions = np.array([],dtype=str)\n",
    "\n",
    "for i in df_max['DESTINO']:            \n",
    "        if i not in differents_directions:\n",
    "                differents_directions = np.append(differents_directions,i)\n",
    "        else:\n",
    "            same_directions = np.append(same_directions,i)  \n",
    "            #print('ten en cuenta que la dirección %s tiene más de una entrega' % same_direction)\n",
    "            \n",
    "# REMOVIDO DE ENTREGAS DUPLICADAS EN EL MISMO DESTINO PARA LIMPIAR EL DATAFRAME\n",
    "df_max.drop_duplicates(inplace=True)\n",
    "df_max.reset_index(drop=True,inplace=True)\n",
    "len(df_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE MAPS PLATFORM\n",
    "\n",
    "**LOGGIN**: obtengo una cuenta y un proyecto con Google Maps, lo cual me permitirá aprovechar sus utilidades. Hay que epsecificar los servicios que se requeriran ya que son llamadas de API diferenciadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### la API Key la guardo en un docuemtno .py la cual importo, aunque esto no me soluciona que se pueda usar por otros usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from info_tfm import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_google = googlemaps.Client(key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accedo a un REPOSITORIO abierto de desarrolladores de google maps para PYTHON, la cual me brinda la posibilidad de poder trabajar desde Python y acceder a los direferentes servicios Google Maps Platform tiene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repositorio con la documentación: https://googlemaps.github.io/google-maps-services-python/docs/#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTANCE MATRIX \n",
    "\n",
    "#### El primer objetivo es crear la matriz de distancias entre los diferentes destinos de entrega. Google aporta una serie de servicios enre los que se incluye la llamada \"distance matrix\" (con una API especifica para ello). Esta es la que usaré a continuación.  En este servicio Google geocodifica por defecto para poder sacar los siguientes parámetros:\n",
    "\n",
    "- Distancia\n",
    "- Duración\n",
    "- Duración en tráfico (si el transporte es automóvil)\n",
    "- Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlemaps import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodología de llamada a la API de Google \n",
    "\n",
    "**DIFERENCIA CON MODELO 1º**: como vimos en el Modelo 1º,  la API de Google no tolera meter más de 100 combinaciones de origenes-destinos (lo que equivale a 10 puntos de reparto), a la par que no tiene limite de peticiones diarias. Es por ello que llamábamos reiteradamente a la API de Google con 10 destinos y obteníamos el menor número de matrices de distancias (y con ello obteníamos los clusters de la clusterización  de 1º grado).\n",
    "\n",
    "Condiciones Google:\n",
    "\n",
    "- Limited to 100 elements per client-side request.\n",
    "- Maximum of 25 origins and 25 destinations per server-side request.\n",
    "- 1,000 server-side elements per second. *Note that the client-side service offers Unlimited elements per second, per project.\n",
    "\n",
    "Para más información: https://developers.google.com/maps/documentation/javascript/distancematrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARÁMETROS DE LA DISTANCE MATRIX DE GOOGLE\n",
    "\n",
    "Siguen siendo los mismo que el primer modelo:\n",
    "\n",
    "- Mode=driving ---> porque nuestros repartos son en vehiculo en su mayor parte\n",
    "- Departure_time = 'now'---> Debido a que la orden de repartos es en la mañana\n",
    "- traffic_model = 'pessimistic'---> para no penalizar al conductor si la ruta sufre más atascos de lo normal\n",
    "- region = '.es'---> usamos la ccTLD (códigos de pais) de España para prevenir duplicidades con direcciones de paises hispanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLE Nº DESTINOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n tiene en cuenta el número de destinos a tomar en cuenta en el modelo. En mi caso cogeré 60 \n",
    "# para ambos modelos por cuestiones económicas y de memoria\n",
    "df= df_max.head(n=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna = df['DESTINO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución al problema de la restricción  de GOOGLE\n",
    "\n",
    "Como decíamos, en el primer modelo nos limitaba el problema de que no se podía sacar una matriz de distancias general para todos los puntos de reparto ya que no se podía superar los 10 destinos por limitación de Google, y por ello no se podían conectar todos los puntos de reparto entre ellos, quedando una matriz de distancias incompleta.\n",
    "\n",
    "##### Metodología 2º Modelo: para solventar esta limitación y obtener una matriz de distancias total con el número de destinos que queramos procedo a iterar cada destino con el resto de destinos excepto con él mismo; guardando los resultados en un nueva array.\n",
    "\n",
    "Es una doble iteración teniendo en cuenta las posiciones de cada destino para no coincidir consigo mismo y si con todos los destinos ajenos. Esta metodología resulta más simple y eficaz que el primer modelo a la par que más lenta y arriesgada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOPS POR CADA ENTREGA\n",
    "\n",
    "new_array_dm = np.array([],dtype=object)\n",
    "\n",
    "for i in range(len(columna)):\n",
    "    \n",
    "    array_loop = np.arange(len(columna))\n",
    "     \n",
    "    for loop in array_loop:\n",
    "        if loop!=i:\n",
    "            \n",
    "            dm = distance_matrix.distance_matrix(client=user_google, origins = columna[i],\n",
    "                                destinations = columna[loop], mode = 'driving',\n",
    "                                departure_time = 'now', traffic_model = 'pessimistic', \n",
    "                                region = '.es')\n",
    "                      \n",
    "            new_array_dm = np.append(new_array_dm,dm)            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_array_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tras revisión de errores compruebo que Google a veces (en una minima proporción) no devuelve duration_in_traffic por lo que no podemos continuar con esta variable, aunque será la variable idónea para nuestro propósito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESANIDAMIENTO DE LA INFORMACIÓN DE GOOGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto tenemos una lista de clusters con el formato sacado directamente de Google. Ese formato presenta una estructura de listas y diccionarios superspuestos de donde tendré que sacar la información para crear mi Dataframe con un formato más homogéneo.\n",
    "\n",
    "Google devuelve las siguientes Keys de donde parten los datos:\n",
    "\n",
    "- origin_addresses\n",
    "- destination_addresses\n",
    "- rows: dentro viene la información relativa a tiempo y duración, así como duración de tráfico y status\n",
    "- status\n",
    "\n",
    "**NOTA**: CUANDO EL STATUS ES *NOT FOUND* NO HAY SALIDA DE LOS PARAMETROS, POR LO QUE NO COINCIDEN LAS LONGITUDES DE LAS SERIES QUE CONFORMAN EL DATAFRAME. POR ELLO CREO UN DATA FRAME INICIAL AL QUE QUITAMOS LAS LINEAS *NOT FOUND* PARA QUE LUEGO PUEDA PASAR LOS OTROS PARÁMETROS (DISTANCE, DURATION Y DURATION IN TRAFFIC) Y SE COLOQUEN EN EL ORDEN DESEADO\n",
    "**NOTA**: como se observa---->  if values != 'Madrid, Spain': esto lo implemento porque mediante la observación veía que en algunas mínimas ocasiones daba problema la reducción que google hacía de las calles a esta expresión, duplicando destinos similares que darán problemas al futuro algoritmo de optimización de rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origen = np.array([],dtype=str)\n",
    "destino = np.array([],dtype=str)\n",
    "distance = np.array([],dtype=int)\n",
    "duration = np.array([],dtype=int)\n",
    "#duration_in_traffic = np.array([],dtype=int)\n",
    "status = np.array([],dtype=str)\n",
    "\n",
    "for i in new_array_dm:\n",
    "    for key,values in list(i.items()):  \n",
    "        if values != 'Madrid, Spain':\n",
    "            if key=='origin_addresses':                 \n",
    "                    origen = np.append(origen,values)                \n",
    "            if key=='destination_addresses':               \n",
    "                    destino = np.append(destino,values)\n",
    "            if key=='rows':\n",
    "                for i in values:\n",
    "                    for key,values in list(i.items()):\n",
    "                        for i in values:                          \n",
    "                            for key,values in list(i.items()):                                                          \n",
    "                                if key=='distance':                                   \n",
    "                                    for key,values in list(values.items()):\n",
    "                                        if key=='value':\n",
    "                                            distance = np.append(distance,values)\n",
    "                                if key=='duration':\n",
    "                                    for key,values in list(values.items()):\n",
    "                                        if key=='value':\n",
    "                                            duration = np.append(duration,values)\n",
    "                                #if key=='duration_in_traffic':                                    \n",
    "                                    #for key,values in list(values.items()):\n",
    "                                        #if key=='value':\n",
    "                                            #duration_in_traffic = np.append(duration_in_traffic,values)\n",
    "                                if key=='status':\n",
    "                                    status = np.append(status,values)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(origen),len(destino),len(status),len(distance),len(duration)#,len(duration_in_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = { 'origen' : origen,\n",
    "            'destino' : destino,       \n",
    "             'status': status\n",
    "              }\n",
    "ddm = pd.DataFrame(dfdata,columns=['origen','destino','status'])\n",
    "ddm = ddm[ ddm['status']!='NOT_FOUND']# Para evitar nulos y posibilitar que la extensión de las arrays coincidan \n",
    "ddm['distance'] = distance\n",
    "ddm['duration'] = duration\n",
    "#ddm['duration_in_traffic'] = duration_in_traffic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(ddm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddm = ddm.filter(len(ddm['destino']) <= 20) #aplicar si da error el algoritmo de optimización ya que desetimaría las\n",
    "#reducciones que Google hace de calles que no son \"NOT FOUND\" pero que tampoco encuentra su destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**var_optimizer:** en este modelo posibilitaré cambiar de inicio la variable a optimizar. Se pretende que esta decisión la tome el jefe de tráfico en la visualización, junto con la fecha de reparto y Nº de trabajadores. Luego veremos que la elección de TABLEAU no fue idónea para poder hacer esto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_optimizer = 'duration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ddm = ddm [(ddm[var_optimizer]!=0)] # No aplicar en este modelo\n",
    "ddm.drop_duplicates(inplace=True)# lo pongo para ver si soluciona algo\n",
    "ddm.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZER: LINKAGE--->\"SINGLE\" (a kind linkage of self creeated for deliveries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### El siguiente paso es optimizar las rutas, en ete caso, ya no dentro de cada cluster de 1º Grado.\n",
    "\n",
    "Como comenté en el 1º Modelo, tras probar AgglomerativeClustering y scipy.cluster.hierarchy tomo una de las decisiones más importantes de mi TFM y es la de intentar crear mi propio algoritmo de optimización y clusterización de las rutas en base a las variables que estimemos (duration, duration in traffic y distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables iniciales\n",
    "    \n",
    "index_origin = ddm['duration'].idxmin()\n",
    "destination = ddm.loc[index_origin][0]\n",
    "df_exit = pd.DataFrame(columns=('origen','destino','distance','duration','duration_in_traffic'))\n",
    "df_exit_error = pd.DataFrame(columns=('origen','destino','distance','duration','duration_in_traffic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(ddm)>0:\n",
    "        try:\n",
    "            index = ddm[ ddm['origen']==destination]['duration'].idxmin()            \n",
    "        except ValueError:\n",
    "            print('Google reduce alguna calle a \"Madrid, Spain\" y corta la cadena del index')\n",
    "            \n",
    "        df_exit = df_exit.append((ddm.loc[index]).T)\n",
    "        origin = ddm.loc[index][0]\n",
    "        destination = ddm.loc[index][1]\n",
    "        ddm.drop(index,inplace=True) \n",
    "        ddm.drop(ddm  [ (ddm['origen']==destination) & (ddm['destino']==origin)].index,inplace=True)\n",
    "        ddm.drop(ddm  [  ddm['destino']==destination].index,inplace=True)\n",
    "        ddm.drop(ddm  [  ddm['destino']==origin].index,inplace=True)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado**: con ello lo que hemos conseguido es una ruta optimizada conectando los destinos por la menor variable escogida. En nuestro caso, la duración.\n",
    "\n",
    "Tras ello cogeré la array de la varible elegida para el proceso posterior de clusterización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array_var = np.array(df_exit[var_optimizer])\n",
    "array_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASE CLUSTERIZACIÓN FINAL\n",
    "\n",
    "En este momento disponemos de un DataFrame ordenado por la variable elegida (duration en nuestro caso) optimizadas anteriormente. La clusterización  Final consiste en reorganizar los clusters atendiendo al tiempo de reparto de los conductores. Es por ello que segmentaremos el dataframe en clusteres atendiendo a la variable **time_required_per_route**\n",
    "\n",
    "Este criterio o variable limitará el número de destinos o rutas por cluster en función de que se cumpla el argumento que queramos. En este vaso, como en el primer modelo, queremos que se ajuste se ajuste al horario de los trabajadores\n",
    "\n",
    "Esta fase es la clave para comprender la finalidad de nuestro modelo,  que tiene tanto una vertiente económica relacionada con la optimización del trabajo de cada repartidor, como decalidad del servicio otorgado\n",
    "\n",
    "#### NOTA: otra var_optimizer a escoger es la *distancia*. Con ello conseguiramos un menor gasto en combustible para los repartos, lo que puede tener un impacto no solo económico sino medioambiental importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUMSUM Y CLUSTERIZACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este apartado procederé de dos manera:\n",
    "\n",
    "- **1) Recomendaremos al jefe de tráfico el Nº de clusters óptimo para un tiempo de ruta dado (time_route)**\n",
    "- **2) El jefe de tráfico dará el número de repartidores que dispone para calcular el Nº de clusters que se ajuste a su petición  y así poder tomar decisiones etratégicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1º)  RECOMENDACIÓN DE CLUSTERS: R_CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición: por defecto siempre dará 21600 segundos = 6 horas de reparto\n",
    "\n",
    "def my_cumsum_func(column_duration,time_route=21600):\n",
    "        grp = np.zeros(len(column_duration))\n",
    "        grp[0] = 0\n",
    "        #dfdata = { 'cumsum_duration' : column_duration,'clusters': grp }\n",
    "\n",
    "        for i in range(1,len(column_duration)):\n",
    "\n",
    "            if (column_duration[i-1] + column_duration[i]) <= time_route:\n",
    "                grp[i] = grp[i-1]\n",
    "                column_duration[i] = column_duration[i-1] + column_duration[i]\n",
    "            else:\n",
    "                grp[i] = grp[i-1] + 1\n",
    "                \n",
    "        result = np.array([column_duration,grp])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_required_per_route = 4000 # prueba con 1 hora aprox\n",
    "array_var_cumsum = my_cumsum_func(array_var,time_required_per_route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completo el DataFrame con los resultados (el cumsum y el nº de cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit['R_cumsum_var']=array_var_cumsum[0]\n",
    "df_exit['R_clusters']=array_var_cumsum[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2º)  SOLICITUD CON Nº REPARTIDORES DADO: N_CLUSTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comenté anteriomente, trato de solicitar el nº de trabajadores usando el módulo visto en clase de *Flask*, pero no consigo obtener el entero y demás variables de manera sencilla, por lo que dejo el Nº trabajadores estático en 8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from flask import Flask\n",
    "\n",
    "app= Flask(\"My work number\")\n",
    "\n",
    "@app.route('/ret_number/<int:n>', methods=['GET'])\n",
    "def get_num(n):\n",
    "    try:\n",
    "        numb=int(n)\n",
    "        return numb\n",
    "    except:\n",
    "        return \"Could not find a number\"\n",
    "    \n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JEFE DE TRÁFICO INDICA Nº DE TRABAJADORES:\n",
    "N_works = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_var_2 = np.array(df_exit[var_optimizer])\n",
    "array_var_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función diferenciada por la petición del Nº de trabajadores\n",
    "def my_Ncumsum_func(column_duration,N_works):\n",
    "    \n",
    "        time_route_estimated = round(column_duration.sum()/N_works)\n",
    "        \n",
    "        grp = np.zeros(len(column_duration))\n",
    "        grp[0] = 0\n",
    "\n",
    "        for i in range(1,len(column_duration)):\n",
    "\n",
    "            if (column_duration[i-1] + column_duration[i]) <= time_route_estimated:\n",
    "                grp[i] = grp[i-1]\n",
    "                column_duration[i] = column_duration[i-1] + column_duration[i]\n",
    "            else:\n",
    "                grp[i] = grp[i-1] + 1\n",
    "                \n",
    "        result = np.array([column_duration,grp])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_route_estimated_proof = round(array_var_2.sum()/N_works)\n",
    "print(array_var_2.sum())\n",
    "time_route_estimated_proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_var_Ncumsum = my_Ncumsum_func(array_var_2,N_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_var_Ncumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrijo el defecto de los últimos repartos para que se incluyan en el cluster \n",
    "array_var_Ncumsum[1]\n",
    "\n",
    "for i in range(len(array_var_Ncumsum[1])):\n",
    "        if array_var_Ncumsum[1][i] > N_works:\n",
    "            array_var_Ncumsum[1][i] = N_works\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array_var_Ncumsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUMENTO DEL DATAFRAME TOTAL CON LOS CLUSTERES Y EL CUMSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit['N_cumsum_var_']=array_var_Ncumsum[0]\n",
    "df_exit['N_clusters']=array_var_Ncumsum[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZACIÓN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El objetivo es obtener la longitud y latitud de los destinos. Para ello se podría hacer con GEOPANDAS, pero tiene limitación del número de peticiones ya que es necesario una KEY, por lo que accedo mediante la API que ya dispongo de GOOGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlemaps import geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoding_array = np.array([],dtype=object)\n",
    "\n",
    "for destino in range(len(df_exit)):\n",
    "    \n",
    "    df_exit_geocode = geocoding.geocode(client=user_google,address=df_exit.iloc[destino]['destino'])\n",
    "    \n",
    "    geocoding_array = np.append(geocoding_array,df_exit_geocode)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:** este proceso es más corto que el de distance matrix pero en esencia similar. Me limito a obtener las localizaciones de Google Maps para los destinos dado y lo estructuro correctamente para pasar a mi Dataframe los valores de la latitud y de la longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoding_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = np.array([],dtype=float)\n",
    "lng = np.array([],dtype=float)\n",
    "\n",
    "for destino in geocoding_array:\n",
    "         for key,values in list(destino.items()):\n",
    "            if key=='geometry':\n",
    "                for key,values in list(values.items()):\n",
    "                    if key=='location':\n",
    "                        for key,values in list(values.items()):\n",
    "                            if key=='lat':\n",
    "                                lat = np.append(lat,values)\n",
    "                            if key=='lng':\n",
    "                                lng = np.append(lng,values)\n",
    "                            \n",
    "lat,lng        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit['lng']=lng\n",
    "df_exit['lat']=lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saco una tercera columna unificando lat-lng. Aunque a priori inecesario la dejamos por si es útil para la visualización en Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit['lng-lat'] = df_exit[['lng','lat']].apply(lambda x : '{}, {}'.format(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORDENACIÓN DE LAS RUTAS\n",
    "\n",
    "El propósito ahora es indicar, con la creación de dos columnas nuevas (una para R_clusters y otra para N_clusters) el orden de reparto de las diferentes destinos dentro de cada cluster (de cada tipo, de ahí que se hagan dos columnas)\n",
    "\n",
    "Para ello nos valdremos de una nueva función **my_order_route** que recoge cada cluster y ordena por cada observación. \n",
    "\n",
    "**Objetivo**: Esta ampliación del DataFrame es fundamental para facilitar la correcta visualización de las rutas en **Tableau** ya que así el jefe de tráfico pueda intuir de un vistazo el orden de reparto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función orden de reparto:\n",
    "def my_order_route(clusters_deliveries):  \n",
    "     \n",
    "        grp = np.zeros(len(clusters_deliveries),dtype=int)\n",
    "        grp[0] = 0\n",
    "\n",
    "        for i in range(1,len(clusters_deliveries)):\n",
    "\n",
    "            if clusters_deliveries[i-1] == clusters_deliveries[i]:\n",
    "                grp[i] = grp[i-1]+1\n",
    "                \n",
    "            else:\n",
    "                grp[i] = 0\n",
    "                \n",
    "        result = np.array([grp])\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Procedo con R_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la variable cogiendo la array de R_clsuters\n",
    "array_var_2 = np.array(df_exit['R_clusters'])\n",
    "# Procedo a pasarle la función de order_route\n",
    "array_order_route = my_order_route(array_var_2)\n",
    "# Paso la columna al DataFrame\n",
    "df_exit['R_order_route'] = array_order_route[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Procedo con N_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_var_3 = np.array(df_exit['N_clusters'])\n",
    "array_N_order_route = my_order_route(array_var_3)\n",
    "df_exit['N_order_route'] = array_N_order_route[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUARDO EL  DATA FRAME EN FORMATO EXCEL\n",
    "\n",
    "Necesario para la visualización en Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_document_excel = df_exit.to_excel('final_document.xls',sheet_name='df_exit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRÁFICAS DE VISUALIZACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Prueba visualización de la Recomendación dada al jefe de tráfico.  \n",
    "Visualización con Scatter para ver la correspondiente agruapación de destinos por cluster y comprobar a simple vista si el modelo es idóneo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurando el tamaño de la figura\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "# le paso contenido \n",
    "plt.scatter(x = df_exit['lat'], y = df_exit['lng'], c=df_exit['R_clusters'], cmap='plasma', linewidths=2)\n",
    "plt.xlabel('Latitud')\n",
    "plt.ylabel('Longitud')\n",
    "plt.suptitle('Clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La finalidad de esta priemra visualización es conocer la estructura de distancias para observas posibles destinos alejados de las entregas urbanas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Prueba visualización con el Nº de repartidores dado por el jefe de tráfico.  \n",
    "Visualización con Scatter para ver la correspondiente agruapación de destinos por cluster y comprobar a simple vista si el modelo es idóneo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Con N trabajadores\n",
    "#Visualización con Scatter para ver la correspondiente agruapación de destinos por cluster\n",
    "\n",
    "# configurando el tamaño de la figura\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "# le paso contenido \n",
    "plt.scatter(x = df_exit['lat'], y = df_exit['lng'], c=df_exit['N_clusters'], cmap='plasma', linewidths=2)\n",
    "plt.xlabel('Latitud')\n",
    "plt.ylabel('Longitud')\n",
    "plt.suptitle('N_Clusters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOPANDAS\n",
    "\n",
    "Para darle una visualización y tratamiento más idóneo al tipo de datos que usamos investigo acerca de GEOPANDAS, lo cual me perimite comprender mejor sobre el tratamiento GIS de mis datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instalo GEOPANDAS\n",
    "- ! pip install geopandas\n",
    "\n",
    "En Windows da problemas al instalar geopandas (a diferencia de en linux). No obstante, no es necesario para nuestro modelo continuar con geopandas ya que sólo tiene finalidad didáctica\n",
    "\n",
    "instalo DESCARTES: necesario para poder plotear un geopanda\n",
    "- ! pip install descartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install descartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import descartes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trato de conseguir un archivo.shape de la Comunidad de Madrid por medio de la web \"https://www.comunidad.madrid/servicios/mapas/geoportal-comunidad-madrid\" sin éxito ya que no permiten su descarga\n",
    "\n",
    "Procedo a contactar telefónicamente y me comentan que no está abierto al público esta información y habría que solictarlo si estudio en una Universidad Pública. Es por ello que trato de encontrar mapa en formato \".shape\" sin éxito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Installing Python Shapefile Library (PyShp)\n",
    "! pip install pyshp: pero no voy a usar este sistema\n",
    "\n",
    "\n",
    "The Python Shapefile Library (pyshp) provides read and write support for the Esri Shapefile format. The Shapefile format is a popular Geographic Information System vector data format created by Esri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Es por ello que procedo a investigar si existe mapa de la Com.de Madrid en la base datasets de GEOPANDAS, \n",
    "#pero sólo está un punto de location de Madrid, por lo que no me sirve de base sobre el que proyectar mis \n",
    "#geolocalizaciones\n",
    "gpd.datasets.available\n",
    "path_geocode = gpd.datasets.get_path('naturalearth_cities')\n",
    "df_geocode = gpd.read_file(path_geocode)\n",
    "df_madrid = df_geocode [ df_geocode['name']=='Madrid']\n",
    "df_madrid_geometry = df_madrid['geometry']\n",
    "df_madrid_geometry.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREACIÓN GEODATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a GeoDataFrame from a DataFrame with coordinates\n",
    "df_exit_geo_data = { 'City' : 'Madrid',\n",
    "             'Country' : 'Spain',       \n",
    "             'Latitude': df_exit['lng'],\n",
    "             'Longitude': df_exit['lat'],\n",
    "             'Clusters':df_exit['R_clusters']\n",
    "              }\n",
    "\n",
    "df_exit_geo = pd.DataFrame(df_exit_geo_data,columns=['City','Country','Latitude','Longitude','Clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exit_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covertimos el DataFRame en GeDataFrame para lo cual hay que especificar la geometría dados una longitud/latitud\n",
    "#Estos puntos generados como geometrías son los que GeoPandas entenderá automáticamente como los puntos para plotear\n",
    "gdf = gpd.GeoDataFrame(df_exit_geo, geometry = gpd.points_from_xy(df_exit_geo.Longitude, df_exit_geo.Latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot(color='red',c=gdf['Clusters'])\n",
    "#fig, ax = plt.subplots(figsize=(5,5))  (opcional)\n",
    "#gdf.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.points_from_xy(x=gdf['Longitude'],y=gdf['Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "gdf.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTADO FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la comlpicada obtención del documento .shape de la provincia de Madrid la visualización definitiva la finiquitaré en TABLEAU\n",
    "\n",
    "#### df_exit: finalmente hemos conseguido un sólo documento con toda la información imprescindible para poder orientar al jefe de tráfico en la toma de decisiones y llevar a Tableau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
